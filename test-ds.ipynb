{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/code/mistral-reasoning-ft/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict\n",
    "from datasets import Dataset, load_dataset\n",
    "import os\n",
    "import re\n",
    "\n",
    "dataset_path = \"openai/gsm8k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    if text is None:\n",
    "        return \" \"\n",
    "    text = text.strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_cot_example(\n",
    "    example: Dict,\n",
    "    tokenizer,\n",
    "):\n",
    "    question = preprocess(example[\"question\"])\n",
    "    attempt = preprocess(example[\"answer\"])\n",
    "\n",
    "    answer_parts = attempt.split(\"####\")\n",
    "    thinking = answer_parts[0].strip()\n",
    "    answer = answer_parts[1].strip() if len(answer_parts) > 1 else \"\"\n",
    "\n",
    "    assistant_text = (\n",
    "        \"[THINK]\\n\"\n",
    "        + thinking\n",
    "        + \"\\n[/THINK]\\n\"\n",
    "        + \"\\n[ANSWER]\\n\"\n",
    "        + answer\n",
    "        + \"\\n[/ANSWER]\\n\"\n",
    "    )\n",
    "\n",
    "    text = tokenizer.apply_chat_template(\n",
    "        [\n",
    "            {\"role\": \"user\", \"content\": question},\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": assistant_text,\n",
    "            },\n",
    "        ],\n",
    "        tokenize=False,\n",
    "    )\n",
    "    return dict(text=text)\n",
    "\n",
    "\n",
    "def preprocess_dataset(\n",
    "    dataset: Dataset,\n",
    "    tokenizer,\n",
    "    processing_function=process_cot_example,\n",
    "):\n",
    "    processed_dataset = dataset.map(\n",
    "        lambda x: processing_function(x, tokenizer),\n",
    "        batched=False,\n",
    "        remove_columns=dataset.column_names,\n",
    "        load_from_cache_file=False,\n",
    "    )\n",
    "    return processed_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_training_dataset(dataset_path: str):\n",
    "    if os.path.exists(dataset_path):\n",
    "        return load_dataset(dataset_path, \"socratic\")\n",
    "    else:\n",
    "        # from hf hub\n",
    "        return load_dataset(dataset_path, \"socratic\")[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "model_id = \"mistralai/Mistral-7B-Instruct-v0.3\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7473/7473 [00:01<00:00, 5422.83 examples/s]\n"
     ]
    }
   ],
   "source": [
    "train_ds = load_training_dataset(dataset_path)\n",
    "processed_train_ds = preprocess_dataset(train_ds, tokenizer, process_cot_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': '<s>[INST] Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?[/INST] [THINK]\\nHow many clips did Natalia sell in May? ** Natalia sold 48/2 = <<48/2=24>>24 clips in May. How many clips did Natalia sell altogether in April and May? ** Natalia sold 48+24 = <<48+24=72>>72 clips altogether in April and May.\\n[/THINK]\\n\\n[ANSWER]\\n72\\n[/ANSWER]</s>'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_token_stats(processed_dataset, tokenizer):\n",
    "    max_tokens = 0\n",
    "    max_idx = 0\n",
    "    total_tokens = 0\n",
    "    min_tokens = float(\"inf\")\n",
    "    count = 0\n",
    "\n",
    "    for idx, example in enumerate(processed_dataset):\n",
    "        # More memory efficient - just get the length without storing tokens\n",
    "        length = len(tokenizer.encode(example[\"text\"], truncation=False))\n",
    "\n",
    "        total_tokens += length\n",
    "        count += 1\n",
    "\n",
    "        if length < min_tokens:\n",
    "            min_tokens = length\n",
    "\n",
    "        if length > max_tokens:\n",
    "            max_tokens = length\n",
    "            max_idx = idx\n",
    "\n",
    "    return {\n",
    "        \"max_tokens\": max_tokens,\n",
    "        \"min_tokens\": min_tokens if min_tokens != float(\"inf\") else 0,\n",
    "        \"avg_tokens\": total_tokens / count if count > 0 else 0,\n",
    "        \"max_idx\": max_idx,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_tokens': 693,\n",
       " 'min_tokens': 107,\n",
       " 'avg_tokens': 256.35835675097013,\n",
       " 'max_idx': 2345}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "check_token_stats(processed_train_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
